# **GPT Language Model**

This repository contains code for building a Large Language Model from scratch using PyTorch, focusing on self-attention and feed-forward layers. The project includes various functionalities such as text encoding, batching, model training, and evaluation.

This project was built following concepts from this [YouTube tutorial](https://www.youtube.com/watch?v=UU1WVnMk4E8).

## **Table of Contents**
- [Requirements](#requirements)
- [Data Preparation](#data-preparation)
- [Model Components](#model-components)
- [Training and Evaluation](#training-and-evaluation)
- [Contributing](#contributing)

## **Requirements**
- Python 3.7+
- PyTorch 1.10+
- A text file (in this case, `the_book_of_wonder.txt`) for training the model

## **Data Preparation**
Ensure that the text file `the_book_of_wonder.txt` is available in the project folder for training the model.

## **Model Components**
The project includes:
- Self-attention layers
- Feed-forward neural network layers
- Text encoding and batching

## **Training and Evaluation**
The model training and evaluation scripts are provided for easy experimentation with different configurations and datasets.

## **Contributing**
Feel free to contribute to this project by creating issues or pull requests.
